---
title: 强化学习的数学原理笔记01
author: icyfish
date: 2025-09-19 19:54:56
tags:
  - 强化学习
categories:
  - 计算机
plugins: 
  - mathjax
---
本章将会聚焦于强化学习中的基本概念，希望可以简单通俗的语言解释清楚何为 状态 (State) 、动作 (Action)、状态空间、行动空间、 状态转移(State transition)、策略 (Policy) 、奖励  (Reward)、轨迹 (Trajectory)、回报 (Return)、 回合 (Eposide)、马尔可夫决策过程 (Markov Decision Process) 这些概念的定义以及基于这几个概念建立的整个 RL 框架。
<!-- more -->

# 强化学习的数学原理 --  Chapter 1 ：基本概念
##  基本概念
### 状态 $s$
智能题对于环境的一个相对状态，所有这些相对状态的集合 $S = \{ s_i\}_{i = 1}^{n}$ 就构成整个**状态空间**。

### 行动 $a$
智能体在状态 $s_{i}$ 下做出的转变到下一个状态的过程所需的行为，所有状态下的所有潜在的动作集合就可称为**行动空间** 
$$\mathcal{A} = \{ a^{j}_i \}^{n_1\ ,\ n_2}_{i=1\ ,\ j=1}$$
其中$n_{1}$、$n_{2}$分别为$S$的状态数和$s$中的所有潜在行动数量。

### 状态转移
由$s_j$通过$a_{1}$转移到$s_{j+1}$的过程就是**状态转移**。
$$
    s_{j} \stackrel{a_i}{\longrightarrow}s_{j+1}\stackrel{a_{i+1}}{\longrightarrow}s_{j+2}\stackrel{a_{i+2}}{\longrightarrow}s_{j+3}\cdots\stackrel{a_{i+z-1}}{\longrightarrow}s_{j+z}
$$
一条状态转移的过程称为**轨迹**（从起始状态至结束状态）正如上面的公式所显示的一般。

### 策略 $\pi$
策略告诉了智能体如何在每个状态下如何采取行动，采取什么样的操作。我们使用条件概率 $\pi(\cdot|\cdot)$ 来决定在特定状态下的采取某个策略的几率。

### 奖励 $r$
奖励很像游戏中的小怪打死后爆的金币和强化素材，这些强化素材会使得玩家在一个构筑体系上坚持自己的想法同时越走越远并完善修改自己的想法。

前个状态 $s_{i}$ 每步行动的 $a_{j}$ 都会获得一个奖励 $r_{z}$ 就像在打败（行动）不同种类的怪物（状态）后，获得的素材一般。有时，奖励的数值只看数值的相对性，数值的绝对值和正负差异实则并非十分重要，甚至没有任何影响（可以详见 Cha.2 的定理$3.6$ --最优策略的不变性）。

例如：智能体越过边界或进入禁区，奖励就是低值甚至负值；智能体进入目标区域，奖励就是高值；智能体普通移动，奖励就是0或者负值（更快达到最短策略）。

### 回报 $G$
回报讲述了每个回合中获得的奖励的总和，每个策略都会生成一个回合的回报，通过回合回报的数值比较，可以定量评判“什么是好策略”？“什么是坏策略”？

我们可以思考一个问题，每个强化学习里面都有一个回合吗？一个回合是否可以是无限的？可能发生的问题是在限制的每个回合的步长中，智能体无法求解到最优策略，或者根本就无法进入目标区域；不设置回合的补偿，无限的持续下去，料想智能体定会发生疯狂刷分的策略，通过不断进出目标区域，来获得无限大的奖励和回报，基于这两者评判策略的算法也就不会收敛，求解问题似乎遥遥无期。

此引入折扣因子是必要的，能够保持每个回合持续性的同时又能使算法收敛，换个说法说，折扣因子的加入使得强化学习的算法的步长（每个策略的强度）可以拥有自适应的特点。

引入折扣因子 $\gamma$ 前:
$$
    return = 0 + 0 + 1+1+1+1+\cdots \rightarrow \infty
$$
引入折扣因子 $\gamma$ 后:
$$
    return = 0 + \gamma0 + \gamma^21+\gamma^31+\gamma^41+\gamma^51+\cdots \rightarrow \gamma^2 \cdot \frac{1}{1-\gamma}
$$
回合固定的任务称为回合制任务（episodic task）；回合无限的任务称为持续性任务（continuing task）。两种策略可以将回合制五转化为持续性任务任务：一种设置了一种结束状态为吸收状态，即达到目标区域后直接冻结，只有原地踏步这一个行动选择；另一种将结束状态同其他普通状态一样，引入上文的折扣因子。

### 马尔可夫决策过程 $\mathcal{MDP}$
MDP 是RL问题下的大解决框架，所有问题若希望使用RL方法进行求解，那必须将问题抽象建模为 MDP 框架下的问题。

框架的基本结构可以通过拆词 MDP 得到： 
1. Markov -- 马尔可夫性质（无记忆性）之前的所有状态和行动都不会影响接下来的一步决策。
   
   $p(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) = p(s_{t+1} | s_t, a_t)$

    $p(r_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0) = p(r_{t+1} | s_t, a_t)$
2. Decision -- 策略的选择 $\Pi$ 。
   
3. Process -- 系统模型（预设的状态转移概率 $p(s'|s,a)$ + 奖励概率 $p(r|s,a)$ ）、行动空间、状态空间、奖励组成了过程的背景。

    $\mathcal{S}$、$\mathcal{R(s,a)}$、$\mathcal{A(s)}$

MP V.S MDP，当策略完全确定MDP退化为MP。